{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a6f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import nltk\n",
    "import torch\n",
    "import pymorphy3\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed, hidden, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed)\n",
    "        self.norm2 = nn.LayerNorm(embed)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed, hidden),\n",
    "            nn.GELU(),                 # GELU вместо ReLU\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden, embed)\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # MultiheadAttention принимает key_padding_mask (True = игнорировать токен!)\n",
    "        attn_out, _ = self.attn(x, x, x, key_padding_mask=~mask.bool() if mask is not None else None)\n",
    "        x = self.norm1(x + self.dropout1(attn_out))\n",
    "        x = self.norm2(x + self.dropout2(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class Transformer_Max(nn.Module):\n",
    "    def __init__(self, vocab_size, embed=128, num_classes=17, hidden=1024, num_heads=2, num_layers=2, max_len=500):\n",
    "        super().__init__()\n",
    "        self.tokens = nn.Embedding(vocab_size, embed)\n",
    "        self.poses = nn.Embedding(max_len, embed)\n",
    "        self.transes = nn.ModuleList([\n",
    "            TransformerBlock(embed, hidden, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embed, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch, seq_len = x.shape\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.long, device = x.device).unsqueeze(0).expand_as(x)\n",
    "        x = self.tokens(x) + self.poses(pos)\n",
    "\n",
    "        # маскируем паддинги\n",
    "        mask_with_cls = None if mask is None else mask\n",
    "\n",
    "        # Пропускаем через блоки\n",
    "        for trans in self.transes:\n",
    "            x = trans(x, mask=None if mask is None else mask_with_cls)\n",
    "\n",
    "        x, _ = x.max(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed=128, num_classes=17, hidden=1024, num_heads=2, num_layers=2, max_len=500):\n",
    "        super().__init__()\n",
    "        self.tokens = nn.Embedding(vocab_size+1, embed)\n",
    "        self.poses = nn.Embedding(max_len+1, embed)\n",
    "        self.transes = nn.ModuleList([\n",
    "            TransformerBlock(embed, hidden, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embed, num_classes)\n",
    "        self.cls = vocab_size\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch, seq_len = x.shape\n",
    "        clses = torch.full((batch, 1), self.cls, dtype=torch.long, device=x.device)\n",
    "        x = torch.cat((clses, x), dim=1)\n",
    "        pos = torch.arange(0, seq_len+1, dtype=torch.long, device=x.device).unsqueeze(0).expand_as(x)\n",
    "        x = self.tokens(x) + self.poses(pos)\n",
    "\n",
    "        # маскируем паддинги\n",
    "        mask_with_cls = None if mask is None else torch.cat([torch.ones((batch,1), device=x.device), mask], dim=1)\n",
    "\n",
    "        # Пропускаем через блоки\n",
    "        for trans in self.transes:\n",
    "            x = trans(x, mask=None if mask is None else mask_with_cls)\n",
    "\n",
    "        CLS = x[:, 0, :]\n",
    "        return self.fc(CLS)\n",
    "\n",
    "import dill\n",
    "\n",
    "class Transformer_mix(nn.Module):\n",
    "    def __init__(self, embed=1024, num_classes=17, hidden=2048, num_heads=16, num_layers=8, max_len=500, path_to_vocab=None, q99=120, classes=None, max_segments=100):\n",
    "        super().__init__()\n",
    "        self.vocab = dill.load(open(path_to_vocab, 'rb'))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.tokens = nn.Embedding(self.vocab_size+1, embed, padding_idx=0)\n",
    "        self.poses = nn.Embedding(max_len+1, embed)\n",
    "        self.segments = nn.Embedding(max_segments, embed)\n",
    "        self.embed_dropout = nn.Dropout(0.1)\n",
    "        self.transes = nn.ModuleList(\n",
    "            TransformerBlock(embed, hidden, num_heads) for _ in range(num_layers)\n",
    "        )\n",
    "        self.layernorm = nn.LayerNorm(embed)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed*3, embed*6),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed*6, num_classes)\n",
    "        )\n",
    "        self.cls = self.vocab_size\n",
    "        self.q99 = q99\n",
    "        self.sep_id = self.vocab['<SEP>']\n",
    "        self.classes = classes\n",
    "\n",
    "    def predict(self, text, device):\n",
    "        # Индексы токенов\n",
    "        indices_tr = [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in text.split()]\n",
    "        indices_tr_q99 = indices_tr[:q99]  # обрезка до q99\n",
    "        # Паддинг\n",
    "        pad_len = q99 - len(indices_tr_q99)\n",
    "        indices_tr_q99 = indices_tr_q99 + [self.vocab[\"<PAD>\"]] * pad_len\n",
    "        # Маска: 1 = реальный токен, 0 = паддинг\n",
    "        mask = [1] * (len(indices_tr[:q99])) + [0] * pad_len #indices_tr хранит предложения без падинга, так что по его длине максируем [1] и [0] падинги, но отсекаем по q99, если длинный попался\n",
    "        # В тензоры\n",
    "        x = torch.tensor(indices_tr_q99, dtype=torch.long).unsqueeze(0).to(device)  # (1, q99)\n",
    "        mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0).to(device)       # (1, q99)\n",
    "        # Предсказание\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x, mask)\n",
    "            pred = torch.argmax(logits, dim=1).item()\n",
    "        print(classes[pred])\n",
    "        return pred\n",
    "\n",
    "    def forward(self, x, mask=None, return_hidden=False):\n",
    "        batch, seq = x.shape\n",
    "        clses = torch.full((batch, 1), self.cls, dtype=torch.long, device=x.device)\n",
    "        x = torch.cat([clses, x], dim=1)\n",
    "\n",
    "        pos = torch.arange(0, seq+1, device=x.device).unsqueeze(0).expand(batch, -1)\n",
    "        # сегменты (находим <SEP>, делаем cumsum)\n",
    "        #sep_mask = (x == self.sep_id).int()       # где стоят <SEP>, будет 1\n",
    "        #segments = sep_mask.cumsum(dim=1) % 2 # нумерация сегментов (0,1,2,...) кумулятивной суммой, то есть при каждом попадании единицы будет +1 к сумме, далее значения будут на 1 больше\n",
    "        #segments = segments.clamp(max=self.segments.num_embeddings - 1) #модель рассчитана на 10 предложений, такой передан параметр, поэтому все предложения после 9 будут иметь индекс 9\n",
    "\n",
    "        # складываем эмбеддинги\n",
    "        #x = self.embed_dropout(self.tokens(x) + self.poses(pos) + self.segments(segments))\n",
    "        x = self.embed_dropout(self.tokens(x) + self.poses(pos))\n",
    "        # маскируем паддинги\n",
    "        mask_with_cls = None if mask is None else torch.cat([torch.ones((batch,1), device=x.device), mask], dim=1)\n",
    "\n",
    "        # Пропускаем через блоки\n",
    "        for trans in self.transes:\n",
    "            x = trans(x, mask=None if mask is None else mask_with_cls)\n",
    "            \n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        if return_hidden:\n",
    "            return x[:, 1:, :]\n",
    "        \n",
    "        if mask is not None:\n",
    "            x_masked = x[:,1:,:] * mask.unsqueeze(-1)   # обнуляем паддинги\n",
    "            mean_pooling = x_masked.sum(1) / mask.sum(1, keepdim=True).clamp(min=1)\n",
    "            x_masked_for_max = x[:, 1:, :].masked_fill(mask.unsqueeze(-1) == 0, -1e9) #шумы, прошедшие через модель, по сути фейки, нельзя, чтобы выбрались как максимум вместо реальных данных\n",
    "            max_pooling, _ = x_masked_for_max.max(1)\n",
    "        else:\n",
    "            mean_pooling = x[:,1:,:].mean(1)\n",
    "            max_pooling, _ = x[:,1:,:].max(1)\n",
    "\n",
    "        CLS = x[:,0,:]\n",
    "        out = torch.cat([CLS, mean_pooling, max_pooling], dim=1)\n",
    "        return self.fc(out)\n",
    "\n",
    "class Classificator(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ff(x)\n",
    "        return x\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def train(model, model_name:str, optimizer, loss_fn, train_loader, test_loader, scheduler, epochs, device):\n",
    "    model, loss_fn = model.to(device), loss_fn.to(device)\n",
    "    losses, f1s, accs = [], [], []\n",
    "    best_acc, idx, best_idx, grad_norm = 0, 0, 0, 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, mask, y in train_loader:\n",
    "            x, mask, y = x.to(device), mask.to(device), y.to(device, dtype=torch.long)\n",
    "            pred = model(x, mask)\n",
    "\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # ограничиваем градиенты перед шагом оптимизатора\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        f1, acc, val_loss = validate(model, loss_fn, test_loader, device)\n",
    "        model.train()\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        f1s.append(f1)\n",
    "        accs.append(acc)\n",
    "        losses.append(val_loss)\n",
    "        idx += 1\n",
    "        if acc>best_acc:\n",
    "          torch.save(model.state_dict(), f'weights_{model_name}.pt')\n",
    "          best_acc = acc\n",
    "          best_idx = idx\n",
    "          print(f\"✅ Saved new best model to {f'weights_{model_name}.pt'}\")\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | loss={val_loss:.4f}, f1={f1:.4f}, acc={acc:.4f}\")\n",
    "    print(f'best_epoch {best_idx} with acc {best_acc}')\n",
    "    return losses, f1s, accs\n",
    "\n",
    "\n",
    "def validate(model, loss_fn, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    loss_sum, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, mask, y in test_loader:\n",
    "            x, mask, y = x.to(device), mask.to(device), y.to(device, dtype=torch.long)\n",
    "            pred = model(x, mask)\n",
    "\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            preds = pred.argmax(dim=1)\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            total += y.size(0)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return f1, acc, loss_sum / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd916f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anek_2ch_vocab_20000.pkl', 'rb') as file:\n",
    "    vocab_small = dill.load(file)\n",
    "def tensoring(xtr,xte,ytr,yte, vocab):    \n",
    "    sents_tr = [x.split() for x in xtr.tolist()]\n",
    "    sents_te = [x.split() for x in xte.tolist()]\n",
    "\n",
    "    indices_tr = [[vocab.get(word, vocab[\"<UNK>\"]) for word in sent] for sent in sents_tr]\n",
    "    indices_te = [[vocab.get(word, vocab[\"<UNK>\"]) for word in sent] for sent in sents_te]\n",
    "\n",
    "    length_tr = [len(sent.split()) for sent in xtr.to_list()]\n",
    "    q99 = int(pd.DataFrame(length_tr).quantile(0.99))\n",
    "    print(f\"q99 длины = {q99}\")\n",
    "    indices_tr_q99 = [sent[:q99] for sent in indices_tr if len(sent) > 0]\n",
    "    indices_te_q99 = [sent[:q99] for sent in indices_te if len(sent) > 0]\n",
    "\n",
    "    seqs_tr = [torch.tensor(seq) for seq in indices_tr_q99]\n",
    "    seqs_te = [torch.tensor(seq) for seq in indices_te_q99]\n",
    "\n",
    "    padded_tr = pad_sequence(seqs_tr, batch_first=True, padding_value=vocab[\"<PAD>\"])\n",
    "    padded_te = pad_sequence(seqs_te, batch_first=True, padding_value=vocab[\"<PAD>\"])\n",
    "\n",
    "    ytr_tensor = torch.tensor(ytr.values, dtype=torch.long)\n",
    "    yte_tensor = torch.tensor(yte.values, dtype=torch.long)\n",
    "\n",
    "    # attention_mask: 1 для токена, 0 для паддинга\n",
    "    tr_mask = (padded_tr != 0).long()\n",
    "    te_mask = (padded_te != 0).long()\n",
    "\n",
    "    train_set2 = TensorDataset(padded_tr, tr_mask, ytr_tensor)\n",
    "    test_set2 = TensorDataset(padded_te, te_mask, yte_tensor)\n",
    "\n",
    "    train_loader2 = DataLoader(train_set2, batch_size=100)\n",
    "    test_loader2 = DataLoader(test_set2, batch_size=100)\n",
    "    return train_loader2, test_loader2\n",
    "def preprocess(text):\n",
    "    # токенизация (только слова, без пунктуации)\n",
    "    tokens = word_tokenize(text, language=\"russian\")\n",
    "    # лемматизация + удаление стоп-слов и чисел\n",
    "    sep_tokens = {\".\", \"?\", \"!\", \"...\"}\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        if token in sep_tokens:\n",
    "            lemmas.append(\"<SEP>\")\n",
    "        elif token.isalpha() and token not in stop_words:\n",
    "            lemmas.append(morph.parse(token)[0].normal_form)\n",
    "\n",
    "    return \" \".join(lemmas)\n",
    "def prepare():\n",
    "    df = pd.read_csv('m18_jokes_dataset.csv')\n",
    "\n",
    "    df['anekdot'] = df['text'].apply(preprocess)\n",
    "    df.drop(['text'], axis=1, inplace=True)\n",
    "\n",
    "    df = df[~df['anekdot'].isna()]\n",
    "    df = df[df['anekdot'].str.strip() != \"\"]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['temi'] = le.fit_transform(df['theme'])\n",
    "    df.drop(['theme'],axis=1, inplace=True)\n",
    "    xtr, xte, ytr, yte = train_test_split(df['anekdot'], df['temi'], test_size=.1, random_state=42, stratify=df['temi'], shuffle=True)\n",
    "    return xtr, xte, ytr, yte\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "stop_words = set(stopwords.words(\"russian\")) - {'не', 'ну', 'вот'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1341306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q99 длины = 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wa-Arakelyan_P\\AppData\\Local\\Temp\\ipykernel_22156\\2358224339.py:11: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  q99 = int(pd.DataFrame(length_tr).quantile(0.99))\n"
     ]
    }
   ],
   "source": [
    "xtr, xte, ytr, yte = prepare()\n",
    "with open('anek_2ch_vocab_20000.pkl', 'rb') as file:\n",
    "    vocab_small = dill.load(file)\n",
    "# if '<MASK>' not in vocab_small:\n",
    "#     vocab_small['<MASK>'] = len(vocab_small)\n",
    "train_loader3, test_loader3 = tensoring(xtr,xte,ytr,yte, vocab_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8eadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wa-Arakelyan_P\\AppData\\Local\\Temp\\ipykernel_17272\\4227232397.py:78: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  quantile99 = int(pd.DataFrame(lengths).quantile(0.99))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import math\n",
    "\n",
    "with open('df_2ch.pkl', 'rb') as f:\n",
    "    df_2ch = dill.load(f)\n",
    "\n",
    "with open('xtr_with_2ch.pkl', 'rb') as f:\n",
    "    xtr_with_2ch = dill.load(f)\n",
    "\n",
    "with open('xte.pkl', 'rb') as f:\n",
    "    xte = dill.load(f)\n",
    "\n",
    "with open('anek_2ch_vocab_20000.pkl', 'rb') as file:\n",
    "    vocab_small = dill.load(file)\n",
    "    \n",
    "# with open('anek_2ch_vocab.pkl', 'rb') as file:\n",
    "#     vocab_with_2ch = dill.load(file)\n",
    "\n",
    "if '<MASK>' not in vocab_small:\n",
    "    vocab_small['<MASK>'] = len(vocab_small)\n",
    "mask_id = vocab_small['<MASK>']\n",
    "pad_id = vocab_small['<PAD>']\n",
    "sep_id = vocab_small['<SEP>']\n",
    "cls_id = len(vocab_small)\n",
    "\n",
    "def mask_tokens(batch_ids, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    На вход подаётся батч индексов (batch_size, seq_len)\n",
    "    Возвращает:\n",
    "      - masked: тот же тензор, где 15% токенов заменены на <MASK>\n",
    "      - labels: тот же тензор, но все НЕзамаскированные позиции = -100 (игнорируются в CrossEntropyLoss)\n",
    "    \"\"\"\n",
    "    device = batch_ids.device     # берём устройство входного батча\n",
    "    masked = batch_ids.clone()    # копия входов\n",
    "    labels = batch_ids.clone()    # копия входов для обучения\n",
    "    rand = torch.rand(batch_ids.shape, device=device)    # равномерное распределение [0,1)\n",
    "    # приводим идентификаторы к тензорам на том же устройстве\n",
    "    pad_t = torch.tensor(pad_id, device=device)\n",
    "    cls_t = torch.tensor(cls_id, device=device)\n",
    "    sep_t = torch.tensor(sep_id, device=device)\n",
    "\n",
    "    # Выбираем токены, которые заменим на MASK (15%)\n",
    "    mask_arr = (rand < mask_prob) & (batch_ids != pad_t) & (batch_ids != cls_t) & (batch_ids != sep_t)\n",
    "\n",
    "    # распределение как в BERT\n",
    "    # 80% токенов <MASK>\n",
    "    mask_mask = (torch.rand(batch_ids.shape, device=device) < 0.8) & mask_arr\n",
    "    masked[mask_mask] = mask_id\n",
    "\n",
    "    # 10% случайные токены\n",
    "    rand_replace = (torch.rand(batch_ids.shape, device=device) < 0.1) & mask_arr & ~mask_mask\n",
    "    random_words = torch.randint(len(vocab_small), batch_ids.shape, device=device)\n",
    "    masked[rand_replace] = random_words[rand_replace]\n",
    "\n",
    "    # 10% остаются как есть (уже handled автоматически)\n",
    "\n",
    "    # Все не замаскированные метки делаем -100 (чтобы не учитывались в loss)\n",
    "    labels[~mask_arr] = -100\n",
    "\n",
    "    return masked, labels  # форма (batch_size, seq_len)\n",
    "\n",
    "# Формируем корпус для самопредобучения: анекдоты + 2ch\n",
    "#texts_unsup = pd.concat([df_2ch, df['anekdot']]).drop_duplicates().tolist()\n",
    "\n",
    "# Замена на индексы\n",
    "tokenized_unsup = xtr_with_2ch.copy()#[[vocab_small.get(token, vocab_small['<UNK>']) for token in text]for text in df_with_2ch]\n",
    "val_set = xte.copy()\n",
    "# Обрезание до q99 и тензоризация\n",
    "lengths = [len(sent) for sent in tokenized_unsup]\n",
    "quantile99 = int(pd.DataFrame(lengths).quantile(0.99))\n",
    "tokenized_unsup = [torch.tensor(sent[:quantile99]) for sent in tokenized_unsup if len(sent)>0]\n",
    "val_set = [torch.tensor(sent[:quantile99]) for sent in val_set if len(sent)>0]\n",
    "# Паддинг до максимальной длины внутри батча\n",
    "padded_unsup = pad_sequence(tokenized_unsup, batch_first=True, padding_value=pad_id)\n",
    "padded_val = pad_sequence(val_set, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "# (batch_size, seq_len) теперь все строки одинаковой длины\n",
    "\n",
    "# Attention mask: 1 для токенов, 0 для паддингов\n",
    "mask_unsup = (padded_unsup != pad_id).long()\n",
    "mask_val = (padded_val != pad_id).long()\n",
    "# (batch_size, seq_len)\n",
    "\n",
    "# Делаем датасет\n",
    "unsup_set = TensorDataset(padded_unsup, mask_unsup)\n",
    "unsup_loader = DataLoader(unsup_set, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_tenset = TensorDataset(padded_val, mask_val)\n",
    "val_loader = DataLoader(val_tenset, batch_size=32, num_workers=2)\n",
    "# В каждом батче:\n",
    "#   x.shape = (64, seq_len)\n",
    "#   mask.shape = (64, seq_len)\n",
    "\n",
    "# Добавляем новую \"голову\" для MLM. У модели добавится второй выход. Первый так и остается для классификации, второй для восстановления замаскированных токенов\n",
    "\n",
    "# У модели Transformer_mix выход из self.fc имеет вид (batch_size, num_classes)\n",
    "# Но нам нужен выход по токенам для каждого токена предсказать слово.\n",
    "# Поэтому добавляем линейный слой fc_mlm, который будет восстанавливать исходный токен.\n",
    "# Его вход скрытое представление каждого токена (embed_dim), выход размер словаря (len(vocab)).\n",
    "model_class_mix2 = Transformer_mix(len(vocab_small), q99=quantile99, path_to_vocab='anek_2ch_vocab_20000.pkl')\n",
    "embed = model_class_mix2.tokens.embedding_dim\n",
    "model_class_mix2.fc_mlm = nn.Sequential(nn.Dropout(0.1), nn.Linear(embed, len(vocab_small)))\n",
    "# аккуратно копируем веса (без CLS-вектора)\n",
    "with torch.no_grad():\n",
    "    model_class_mix2.fc_mlm[1].weight.copy_(model_class_mix2.tokens.weight[:-1]) # копируем веса эмбеддингов словаря из этапа \"чтения модели\" в веса этапа \"угадывания токенов\", потому что это действие обратное пониманию. Linear лежит вторым в Sequential\n",
    "\n",
    "for p in model_class_mix2.fc.parameters():\n",
    "    p.requires_grad = False # замораживаем веса для классификатора, мы обучаем вторую голову\n",
    "\n",
    "# fc[0].in_features // 3 потому что в модели fc принимает concat из CLS + mean + max\n",
    "#   (embed * 3), так что одно \"ядро\" embed\n",
    "# Размеры:\n",
    "#   logits.shape = (batch_size, seq_len, vocab_size)\n",
    "# теперь модель высчитывает два результата, она годится и для классификации, и для генерации текста, по желанию можно вызвать результат model.fc через model(x, attn_mask), это основной выход, потому что он указан в return, или вернуть model.fc_mlm через через model(x, attn_mask, return_hidden=True)\n",
    "\n",
    "mlm_criterion = nn.CrossEntropyLoss(ignore_index=-100)  # не считаем loss на не-масках\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\", \"layernorm.weight\", \"norm.weight\"]\n",
    "param_groups = [\n",
    "    {\"params\": [p for n,p in model_class_mix2.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": 5e-5},\n",
    "    {\"params\": [p for n,p in model_class_mix2.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=5e-5, betas=(0.9, 0.98), eps=1e-8)\n",
    "\n",
    "# Обучение Masked Language Model\n",
    "epochs = 16\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=4000, num_training_steps=len(unsup_loader)*epochs\n",
    ")\n",
    "\n",
    "def train_mlm(model, loss_fn, optimizer, train_loader, val_loader, device, scheduler, epochs=5):\n",
    "    \n",
    "    # Настройка scheduler\n",
    "    # num_training_steps = len(train_loader) * epochs\n",
    "    # num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "    \n",
    "    # Подготовка \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Предсоздание маскированной валидации (фиксированной). Мы должны использовать одинаково замаскированные входы на треине и валидации для стабильности обучения\n",
    "    val_data = []\n",
    "    for x, attn_mask in val_loader:\n",
    "        x, attn_mask = x.to(device), attn_mask.to(device)\n",
    "        masked_x, labels = mask_tokens(x)\n",
    "        val_data.append((x, attn_mask, masked_x, labels))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, total_acc, grad_norm_sum = 0, 0, 0\n",
    "\n",
    "        for x, attn_mask in tqdm(train_loader, desc=f\"MLM Epoch {epoch+1}/{epochs}\"):\n",
    "            x, attn_mask = x.to(device), attn_mask.to(device)\n",
    "            masked_x, labels = mask_tokens(x)\n",
    "\n",
    "            # Forward\n",
    "            hiddens = model(masked_x, attn_mask, return_hidden=True)\n",
    "            logits = model.fc_mlm(hiddens)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            grad_norm_sum += grad_norm.item()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Метрики \n",
    "            total_loss += loss.item()\n",
    "            with torch.no_grad():\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                mask_pos = labels != -100\n",
    "                total_acc += (preds[mask_pos] == labels[mask_pos]).float().mean().item()\n",
    "        print(\"lr=\", optimizer.param_groups[0][\"lr\"])\n",
    "        # Усреднение\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_acc = total_acc / len(train_loader)\n",
    "        avg_grad = grad_norm_sum / len(train_loader)\n",
    "        print(f\"Train | Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | MaskedAcc: {avg_acc:.4f} | GradNorm: {avg_grad:.3f}\")\n",
    "\n",
    "        # Валидация\n",
    "        val_loss, val_acc = validate_mlm(model, loss_fn, val_data, device)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'weights_mlm_1024.pt')\n",
    "            print(f\"✅ Saved new best model (val_acc={val_acc:.4f})\")\n",
    "\n",
    "    print(f\"\\nTraining finished. Best val_acc: {best_val_acc:.4f}, Final_loss {val_loss}\")\n",
    "    \n",
    "def validate_mlm(model, loss_fn, val_data, device):\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, attn_mask, masked_x, labels in tqdm(val_data, desc=\"Validating\"):\n",
    "            hiddens = model(masked_x, attn_mask, return_hidden=True)\n",
    "            logits = model.fc_mlm(hiddens)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask_pos = labels != -100\n",
    "            total_acc += (preds[mask_pos] == labels[mask_pos]).float().mean().item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_data)\n",
    "    avg_acc = total_acc / len(val_data)\n",
    "    print(f\"Validation | Loss: {avg_loss:.4f} | MaskedAcc: {avg_acc:.4f}\")\n",
    "    model.train()\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d6c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 1/16: 100%|██████████| 11234/11234 [26:09<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 4.7941892753095415e-05\n",
      "Train | Epoch 1/16 | Loss: 40.0899 | MaskedAcc: 0.1874 | GradNorm: 89.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:03<00:00, 17.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 16.7842 | MaskedAcc: 0.3065\n",
      "✅ Saved new best model (val_acc=0.3065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 2/16: 100%|██████████| 11234/11234 [27:45<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 4.474576656955572e-05\n",
      "Train | Epoch 2/16 | Loss: 12.2023 | MaskedAcc: 0.2311 | GradNorm: 38.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:03<00:00, 17.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 6.0039 | MaskedAcc: 0.3083\n",
      "✅ Saved new best model (val_acc=0.3083)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 3/16: 100%|██████████| 11234/11234 [27:39<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 4.154964038601602e-05\n",
      "Train | Epoch 3/16 | Loss: 5.9601 | MaskedAcc: 0.3040 | GradNorm: 23.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:03<00:00, 17.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.8351 | MaskedAcc: 0.3093\n",
      "✅ Saved new best model (val_acc=0.3093)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 4/16: 100%|██████████| 11234/11234 [27:42<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 3.835351420247633e-05\n",
      "Train | Epoch 4/16 | Loss: 5.8094 | MaskedAcc: 0.3071 | GradNorm: 19.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:02<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.7066 | MaskedAcc: 0.3075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 5/16: 100%|██████████| 11234/11234 [27:36<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 3.515738801893664e-05\n",
      "Train | Epoch 5/16 | Loss: 5.7423 | MaskedAcc: 0.3088 | GradNorm: 17.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:03<00:00, 17.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.6294 | MaskedAcc: 0.3063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 6/16: 100%|██████████| 11234/11234 [27:36<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 3.1961261835396946e-05\n",
      "Train | Epoch 6/16 | Loss: 5.6771 | MaskedAcc: 0.3110 | GradNorm: 16.886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:02<00:00, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.5630 | MaskedAcc: 0.3106\n",
      "✅ Saved new best model (val_acc=0.3106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 7/16: 100%|██████████| 11234/11234 [27:41<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 2.876513565185725e-05\n",
      "Train | Epoch 7/16 | Loss: 5.6368 | MaskedAcc: 0.3117 | GradNorm: 17.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:03<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.4825 | MaskedAcc: 0.3120\n",
      "✅ Saved new best model (val_acc=0.3120)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 8/16: 100%|██████████| 11234/11234 [27:36<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 2.5569009468317557e-05\n",
      "Train | Epoch 8/16 | Loss: 5.5830 | MaskedAcc: 0.3144 | GradNorm: 15.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:03<00:00, 17.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.4316 | MaskedAcc: 0.3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 9/16: 100%|██████████| 11234/11234 [27:37<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 2.237288328477786e-05\n",
      "Train | Epoch 9/16 | Loss: 5.5377 | MaskedAcc: 0.3157 | GradNorm: 15.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:03<00:00, 17.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.3688 | MaskedAcc: 0.3167\n",
      "✅ Saved new best model (val_acc=0.3167)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 10/16: 100%|██████████| 11234/11234 [27:40<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 1.9176757101238165e-05\n",
      "Train | Epoch 10/16 | Loss: 5.5047 | MaskedAcc: 0.3163 | GradNorm: 15.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:02<00:00, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.3109 | MaskedAcc: 0.3190\n",
      "✅ Saved new best model (val_acc=0.3190)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 11/16: 100%|██████████| 11234/11234 [27:37<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 1.5980630917698473e-05\n",
      "Train | Epoch 11/16 | Loss: 5.4697 | MaskedAcc: 0.3178 | GradNorm: 15.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:02<00:00, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.2691 | MaskedAcc: 0.3208\n",
      "✅ Saved new best model (val_acc=0.3208)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 12/16: 100%|██████████| 11234/11234 [27:41<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 1.2784504734158779e-05\n",
      "Train | Epoch 12/16 | Loss: 5.4487 | MaskedAcc: 0.3178 | GradNorm: 15.088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:02<00:00, 18.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.2397 | MaskedAcc: 0.3203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 13/16: 100%|██████████| 11234/11234 [27:43<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 9.588378550619083e-06\n",
      "Train | Epoch 13/16 | Loss: 5.4266 | MaskedAcc: 0.3186 | GradNorm: 15.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:02<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.2078 | MaskedAcc: 0.3218\n",
      "✅ Saved new best model (val_acc=0.3218)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 14/16: 100%|██████████| 11234/11234 [27:43<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 6.392252367079389e-06\n",
      "Train | Epoch 14/16 | Loss: 5.4059 | MaskedAcc: 0.3190 | GradNorm: 15.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:03<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.1882 | MaskedAcc: 0.3252\n",
      "✅ Saved new best model (val_acc=0.3252)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 15/16: 100%|██████████| 11234/11234 [27:38<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 3.1961261835396947e-06\n",
      "Train | Epoch 15/16 | Loss: 5.3908 | MaskedAcc: 0.3193 | GradNorm: 15.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:03<00:00, 17.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.1692 | MaskedAcc: 0.3251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 16/16: 100%|██████████| 11234/11234 [27:38<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.0\n",
      "Train | Epoch 16/16 | Loss: 5.3763 | MaskedAcc: 0.3200 | GradNorm: 15.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:02<00:00, 18.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation | Loss: 5.1578 | MaskedAcc: 0.3232\n",
      "\n",
      "Training finished. Best val_acc: 0.3252, Final_loss 5.157759339721115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_mlm(model_class_mix2, mlm_criterion, optimizer, unsup_loader, val_loader, device, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fb8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wa-Arakelyan_P\\AppData\\Local\\Temp\\ipykernel_17272\\3940519284.py:8: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  quantile99 = int(pd.DataFrame(lengths).quantile(0.99))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Загружены предобученные MLM-веса\n",
      "✅ Saved new best model to weights_mlm_classification_20000_1024.pt\n",
      "Epoch 1/50 | loss=1.8464, f1=0.4096, acc=0.4459\n",
      "✅ Saved new best model to weights_mlm_classification_20000_1024.pt\n",
      "Epoch 2/50 | loss=1.5415, f1=0.5141, acc=0.5359\n",
      "✅ Saved new best model to weights_mlm_classification_20000_1024.pt\n",
      "Epoch 3/50 | loss=1.4523, f1=0.5501, acc=0.5688\n",
      "✅ Saved new best model to weights_mlm_classification_20000_1024.pt\n",
      "Epoch 4/50 | loss=1.3913, f1=0.5789, acc=0.5976\n",
      "✅ Saved new best model to weights_mlm_classification_20000_1024.pt\n",
      "Epoch 5/50 | loss=1.3659, f1=0.5909, acc=0.6088\n",
      "✅ Saved new best model to weights_mlm_classification_20000_1024.pt\n",
      "Epoch 6/50 | loss=1.3684, f1=0.5971, acc=0.6141\n",
      "✅ Saved new best model to weights_mlm_classification_20000_1024.pt\n",
      "Epoch 7/50 | loss=1.3841, f1=0.6032, acc=0.6194\n",
      "Epoch 8/50 | loss=1.4031, f1=0.5987, acc=0.6118\n",
      "Epoch 9/50 | loss=1.4215, f1=0.6001, acc=0.6106\n",
      "Epoch 10/50 | loss=1.4373, f1=0.5934, acc=0.6041\n",
      "Epoch 11/50 | loss=1.4488, f1=0.5972, acc=0.6071\n",
      "Epoch 12/50 | loss=1.4558, f1=0.6005, acc=0.6100\n",
      "Epoch 13/50 | loss=1.4609, f1=0.6016, acc=0.6106\n",
      "Epoch 14/50 | loss=1.4636, f1=0.6021, acc=0.6100\n",
      "Epoch 15/50 | loss=1.4671, f1=0.6011, acc=0.6088\n",
      "Epoch 16/50 | loss=1.4690, f1=0.5990, acc=0.6076\n",
      "Epoch 17/50 | loss=1.4694, f1=0.6006, acc=0.6082\n",
      "Epoch 18/50 | loss=1.4705, f1=0.6034, acc=0.6112\n",
      "Epoch 19/50 | loss=1.4715, f1=0.6014, acc=0.6094\n",
      "Epoch 20/50 | loss=1.4719, f1=0.6025, acc=0.6106\n",
      "Epoch 21/50 | loss=1.4724, f1=0.6028, acc=0.6106\n",
      "Epoch 22/50 | loss=1.4727, f1=0.6027, acc=0.6106\n",
      "Epoch 23/50 | loss=1.4732, f1=0.6019, acc=0.6100\n",
      "Epoch 24/50 | loss=1.4734, f1=0.6026, acc=0.6106\n",
      "Epoch 25/50 | loss=1.4735, f1=0.6026, acc=0.6106\n",
      "Epoch 26/50 | loss=1.4736, f1=0.6020, acc=0.6100\n",
      "Epoch 27/50 | loss=1.4740, f1=0.6026, acc=0.6106\n",
      "Epoch 28/50 | loss=1.4742, f1=0.6026, acc=0.6106\n",
      "Epoch 29/50 | loss=1.4743, f1=0.6026, acc=0.6106\n",
      "Epoch 30/50 | loss=1.4746, f1=0.6026, acc=0.6106\n",
      "Epoch 31/50 | loss=1.4747, f1=0.6026, acc=0.6106\n",
      "Epoch 32/50 | loss=1.4749, f1=0.6026, acc=0.6106\n",
      "Epoch 33/50 | loss=1.4750, f1=0.6020, acc=0.6100\n",
      "Epoch 34/50 | loss=1.4753, f1=0.6026, acc=0.6106\n",
      "Epoch 35/50 | loss=1.4755, f1=0.6026, acc=0.6106\n",
      "Epoch 36/50 | loss=1.4756, f1=0.6020, acc=0.6100\n",
      "Epoch 37/50 | loss=1.4759, f1=0.6020, acc=0.6100\n",
      "Epoch 38/50 | loss=1.4759, f1=0.6021, acc=0.6100\n",
      "Epoch 39/50 | loss=1.4760, f1=0.6021, acc=0.6100\n",
      "Epoch 40/50 | loss=1.4761, f1=0.6021, acc=0.6100\n",
      "Epoch 41/50 | loss=1.4763, f1=0.6021, acc=0.6100\n",
      "Epoch 42/50 | loss=1.4765, f1=0.6027, acc=0.6106\n",
      "Epoch 43/50 | loss=1.4768, f1=0.6021, acc=0.6100\n",
      "Epoch 44/50 | loss=1.4770, f1=0.6021, acc=0.6100\n",
      "Epoch 45/50 | loss=1.4772, f1=0.6021, acc=0.6100\n",
      "Epoch 46/50 | loss=1.4773, f1=0.6021, acc=0.6100\n",
      "Epoch 47/50 | loss=1.4774, f1=0.6021, acc=0.6100\n",
      "Epoch 48/50 | loss=1.4776, f1=0.6021, acc=0.6100\n",
      "Epoch 49/50 | loss=1.4777, f1=0.6021, acc=0.6100\n",
      "Epoch 50/50 | loss=1.4779, f1=0.6027, acc=0.6106\n",
      "best_epoch 7 with acc 0.6194117647058823\n"
     ]
    }
   ],
   "source": [
    "with open('anek_2ch_vocab_20000.pkl', 'rb') as file:\n",
    "    vocab_small = dill.load(file)\n",
    "# if '<MASK>' not in vocab_small:\n",
    "#     vocab_small['<MASK>'] = len(vocab_small)\n",
    "with open('xtr_with_2ch.pkl', 'rb') as file:\n",
    "    xtr_with_2ch = dill.load(file)\n",
    "lengths = [len(sent) for sent in xtr_with_2ch]\n",
    "quantile99 = int(pd.DataFrame(lengths).quantile(0.99))\n",
    "model_class_mix2 = Transformer_mix(len(vocab_small), q99=quantile99, path_to_vocab='anek_2ch_vocab_20000.pkl')\n",
    "# embed = model_class_mix2.tokens.embedding_dim\n",
    "# model_class_mix2.fc_mlm = nn.Linear(embed, len(vocab_small))\n",
    "# аккуратно копируем веса (без CLS-вектора)\n",
    "# with torch.no_grad():\n",
    "#     model_class_mix2.fc_mlm.weight.copy_(model_class_mix2.tokens.weight[:-1]) # копируем веса эмбеддингов словаря из этапа \"чтения модели\" в веса этапа \"угадывания токенов\", потому что это действие обратное пониманию\n",
    "# Загружаем предобученные веса\n",
    "# Загрузить state_dict\n",
    "state = torch.load(\"weights_mlm_1024.pt\", map_location=\"cpu\")\n",
    "\n",
    "# Удалить последний (<MASK>) вектор из весов, потому что эта голова была временной для обучения MLM, для классификации она не нужна и в оригинальном классе ее нет\n",
    "for key in [\"tokens.weight\", \"fc_mlm.weight\", \"fc_mlm.bias\"]:\n",
    "    if key in state:\n",
    "        if key.endswith(\".weight\"):\n",
    "            state[key] = state[key][:-1, :]\n",
    "        elif key.endswith(\".bias\"):\n",
    "            state[key] = state[key][:-1]\n",
    "model_class_mix2.load_state_dict(state, strict=False)\n",
    "print(\"✅ Загружены предобученные MLM-веса\")\n",
    "# Замораживаем эмбеддинги\n",
    "for p in model_class_mix2.tokens.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model_class_mix2.poses.parameters():\n",
    "    p.requires_grad = False\n",
    "# Размораживаем классификатор fc\n",
    "for p in model_class_mix2.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "# Размораживаем последние 3 слоя энкодера\n",
    "for layer in model_class_mix2.transes[-3:]:\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_fn_mix = nn.CrossEntropyLoss()\n",
    "optimizer_mix = torch.optim.Adam(model_class_mix2.parameters(), lr=1e-5, weight_decay=1e-4) #L2 регуляризация\n",
    "scheduler_mix = ReduceLROnPlateau(optimizer_mix, mode='min', factor=0.5, patience=1)\n",
    "epochs = 50\n",
    "losses_mix, f1s_mix, accs_mix = train(model_class_mix2, 'mlm_classification_20000_1024', optimizer_mix, loss_fn_mix, train_loader3, test_loader3, scheduler_mix, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wa-Arakelyan_P\\AppData\\Local\\Temp\\ipykernel_22156\\274880594.py:13: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  q99 = int(pd.DataFrame(lengths).quantile(0.99))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dill\n",
    "import pandas as pd\n",
    "\n",
    "# Загружаем словарь\n",
    "with open('anek_2ch_vocab_20000.pkl', 'rb') as file:\n",
    "    vocab_small_mlm = dill.load(file)\n",
    "if '<MASK>' not in vocab_small_mlm:\n",
    "    vocab_small_mlm['<MASK>'] = len(vocab_small_mlm)\n",
    "\n",
    "lengths = [len(sent) for sent in dill.load(open('xtr_with_2ch.pkl', 'rb'))]\n",
    "q99 = int(pd.DataFrame(lengths).quantile(0.99))\n",
    "\n",
    "model_mlm = Transformer_mix(\n",
    "    vocab_size=len(vocab_small_mlm),\n",
    "    embed=1024,\n",
    "    hidden=2048,\n",
    "    num_heads=16,\n",
    "    num_layers=8,\n",
    "    q99=q99,\n",
    "    path_to_vocab='anek_2ch_vocab_20000.pkl'\n",
    ")\n",
    "\n",
    "# Добавляем голову MLM \n",
    "embed_dim = model_mlm.tokens.embedding_dim\n",
    "model_mlm.fc_mlm = nn.Sequential(\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(embed_dim, len(vocab_small_mlm))\n",
    ")\n",
    "\n",
    "# Загружаем веса MLM\n",
    "state = torch.load(\"weights_mlm_1024.pt\", map_location=\"cpu\")\n",
    "model_mlm.load_state_dict(state, strict=False)\n",
    "model_mlm.eval()\n",
    "\n",
    "import re\n",
    "\n",
    "def inf_preprocess(text):\n",
    "    # заменяем спецтокены временными плейсхолдерами\n",
    "    text = re.sub(r\"<MASK>\", \" MASKTOKEN \", text)\n",
    "    text = re.sub(r\"<SEP>\", \" SEPTOKEN \", text)\n",
    "\n",
    "    tokens = word_tokenize(text, language=\"russian\")\n",
    "    sep_tokens = {\".\", \"?\", \"!\", \"...\"}\n",
    "    lemmas = []\n",
    "\n",
    "    for token in tokens:\n",
    "        # возвращаем спецтокены обратно\n",
    "        if token == \"MASKTOKEN\":\n",
    "            lemmas.append(\"<MASK>\")\n",
    "        elif token == \"SEPTOKEN\":\n",
    "            lemmas.append(\"<SEP>\")\n",
    "        elif token.isalpha() and token not in stop_words:\n",
    "            lemmas.append(morph.parse(token)[0].normal_form)\n",
    "\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Функция инференса\n",
    "def mlm_infer(model, input_text, vocab, device, top_k=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text = inf_preprocess(input_text)\n",
    "        tokens = text.split()\n",
    "        ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
    "        x = torch.tensor(ids).unsqueeze(0).to(device)\n",
    "        mask = (x != vocab[\"<PAD>\"]).long()\n",
    "\n",
    "        hiddens = model(x, mask, return_hidden=True)\n",
    "        logits = model.fc_mlm(hiddens)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        inv_vocab = {v: k for k, v in vocab.items()}\n",
    "        restored = []\n",
    "        candidates = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == \"<MASK>\":\n",
    "                top_preds = torch.topk(probs[0, i], k=top_k)\n",
    "                candidates = [inv_vocab[idx.item()] for idx in top_preds.indices if idx != vocab['<UNK>']]\n",
    "                restored.append(f\"[{', '.join(candidates)}]\")\n",
    "            elif token == \"<SEP>\":  \n",
    "                restored.append('.')\n",
    "            else:\n",
    "                restored.append(token)\n",
    "        preprocessed_answer = \" \".join(restored)\n",
    "        return re.sub(r\"<MASK>\", str(candidates), input_text)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_mlm = model_mlm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c80388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я сегодня отдохнул хорошо. Я завтра буду работать весь ['день', 'быть', 'не', 'мир'] чтобы закончить проект\n"
     ]
    }
   ],
   "source": [
    "text = \"Я сегодня отдохнул хорошо. Я завтра буду работать весь <MASK> чтобы закончить проект\"\n",
    "text2 = \"Прошлый <MASK> был ужасен, я ничего не добился с января по декабрь\"\n",
    "text3 = 'Какой хороший <MASK>. Я заценил и поставил высокую <MASK>'\n",
    "text4 = 'на горе стоит статуя у статуи нету <MASK>'\n",
    "text5 = 'Я смотрел новый <MASK> в кинотеатре.'\n",
    "text6 = 'Сегодня по телевизору опять покажут новый <MASK>'\n",
    "print(mlm_infer(model_mlm, text, vocab_small_mlm, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2b71d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsitati\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_classification = Transformer_mix(len(vocab_small), path_to_vocab='anek_2ch_vocab_20000.pkl')\n",
    "model_classification.load_state_dict(torch.load('weights_mlm_classification_20000_1024.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_fn_mix = nn.CrossEntropyLoss()\n",
    "model_classification = model_classification.to(device)\n",
    "classes = {0: 'aforizmi',\n",
    "            1: 'meditsinskie',\n",
    "            2: 'narodnie',\n",
    "            3: 'poshlie-i-intimnie',\n",
    "            4: 'pro-alkogolikov',\n",
    "            5: 'pro-armiu',\n",
    "            6: 'pro-detey',\n",
    "            7: 'pro-evreev',\n",
    "            8: 'pro-militsiyu',\n",
    "            9: 'pro-mugchin',\n",
    "            10: 'pro-novih-russkih',\n",
    "            11: 'pro-semyu',\n",
    "            12: 'pro-studentov',\n",
    "            13: 'pro-vovochku',\n",
    "            14: 'raznie',\n",
    "            15: 'shkolnie-i-pro-shkolu',\n",
    "            16: 'tsitati'}\n",
    "anek = 'Черчилль спрашивает Сталина что бы вы с вашей армией сделали с Гитлером, будь он у вас в руках? Сталин отвечает раскалил бы кочергу докрасна и засунул бы холодным концом ему в задницу. -А почему холодным, товарищ Сталин? -Чтобы вы, господин Черчилль, не помогли ему ее вытащить'\n",
    "anek2 = 'Приходит пациент к доктору и говорит: доктор, хуй чешется. Доктор отвечает: мой чаще. -Нет, мой'\n",
    "anek3 = \"— Вовочка, ты почему не сделал домашнее задание? — А я в шахматы играл с папой.\"\n",
    "model_classification.predict(anek3, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d46c894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-militsiyu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "shkolnie-i-pro-shkolu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "shkolnie-i-pro-shkolu\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-armiu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-militsiyu\n",
      "tsitati\n",
      "tsitati\n",
      "pro-militsiyu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-militsiyu\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "pro-militsiyu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-armiu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "pro-armiu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-militsiyu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-militsiyu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "pro-armiu\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-armiu\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "shkolnie-i-pro-shkolu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-studentov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-armiu\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "raznie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "shkolnie-i-pro-shkolu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-militsiyu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-armiu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "raznie\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "shkolnie-i-pro-shkolu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "shkolnie-i-pro-shkolu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "pro-militsiyu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "narodnie\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "pro-militsiyu\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-studentov\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n",
      "tsitati\n",
      "aforizmi\n",
      "tsitati\n",
      "tsitati\n",
      "pro-detey\n",
      "tsitati\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('m18_jokes_dataset.csv')\n",
    "data = df['text'].sample(1000)\n",
    "preds_mix = [model_classification.predict(text, device=\"cuda\") for text in data]\n",
    "d = dict(zip(data, [classes[pred] for pred in preds_mix]))\n",
    "dd = pd.DataFrame(list(d.items()), columns=[\"текст\", \"тема\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2401af4",
   "metadata": {},
   "source": [
    "Модель обучилась на разговорном языке с форума 2ch и теперь склонна выбирать анекдотам цитаты, потому что жанр этого языка похож на 2ch.\n",
    "Так что нужно обучить еще пару эпох с заниженным весом для tsitati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c50a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wa-Arakelyan_P\\AppData\\Local\\Temp\\ipykernel_22156\\2358224339.py:11: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  q99 = int(pd.DataFrame(length_tr).quantile(0.99))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q99 длины = 120\n",
      "✅ Saved new best model to weights_mlm_classification_20000_1024_no_tsitati.pt\n",
      "Epoch 1/5 | loss=1.3638, f1=0.5669, acc=0.5847\n",
      "✅ Saved new best model to weights_mlm_classification_20000_1024_no_tsitati.pt\n",
      "Epoch 2/5 | loss=1.4086, f1=0.5653, acc=0.5859\n",
      "Epoch 3/5 | loss=1.4334, f1=0.5623, acc=0.5841\n",
      "Epoch 4/5 | loss=1.4817, f1=0.5581, acc=0.5806\n",
      "Epoch 5/5 | loss=1.5069, f1=0.5594, acc=0.5806\n",
      "best_epoch 2 with acc 0.5858823529411765\n"
     ]
    }
   ],
   "source": [
    "mask = ytr != 16 # \"tsitati\"\n",
    "xtr_sub, ytr_sub = xtr[mask], ytr[mask]\n",
    "train_loader_balanced, _ = tensoring(xtr_sub, xte, ytr_sub, yte, vocab_small) #собрали датасет без цитатных анекдотов, модель сдвинется от склонности присваивать цитаты\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Разморозка нужных частей\n",
    "for p in model_classification.tokens.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model_classification.poses.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model_classification.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "for layer in model_classification.transes[-3:]:\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# Веса классов только по новым классам\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(ytr_sub),  \n",
    "    y=ytr_sub\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device) # у нас 16 новых сбалансированных весов\n",
    "class_weights = torch.cat([class_weights, torch.tensor([0.0], device=device)]) # Поэтому добавим нулевой вес цитатам, иначе не совпадет размерность, классов то 17 изначально\n",
    "\n",
    "# Оптимизатор и loss\n",
    "loss_fn_mix = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_mix = torch.optim.Adam(model_classification.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "scheduler_mix = ReduceLROnPlateau(optimizer_mix, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# Обучение\n",
    "epochs = 5\n",
    "losses_mix, f1s_mix, accs_mix = train(\n",
    "    model_classification,\n",
    "    'mlm_classification_20000_1024_no_tsitati',\n",
    "    optimizer_mix,\n",
    "    loss_fn_mix,\n",
    "    train_loader_balanced,  # новый loader\n",
    "    test_loader3,\n",
    "    scheduler_mix,\n",
    "    epochs,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aforizmi\n",
      "shkolnie-i-pro-shkolu\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-vovochku\n",
      "raznie\n",
      "narodnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "narodnie\n",
      "aforizmi\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-militsiyu\n",
      "narodnie\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "poshlie-i-intimnie\n",
      "aforizmi\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-studentov\n",
      "raznie\n",
      "pro-detey\n",
      "narodnie\n",
      "pro-mugchin\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "raznie\n",
      "pro-armiu\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "pro-studentov\n",
      "narodnie\n",
      "pro-militsiyu\n",
      "pro-detey\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-studentov\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "pro-studentov\n",
      "narodnie\n",
      "pro-militsiyu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-militsiyu\n",
      "poshlie-i-intimnie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "aforizmi\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-detey\n",
      "narodnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "aforizmi\n",
      "raznie\n",
      "raznie\n",
      "pro-studentov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "pro-detey\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "narodnie\n",
      "poshlie-i-intimnie\n",
      "aforizmi\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "narodnie\n",
      "pro-detey\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "shkolnie-i-pro-shkolu\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "aforizmi\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "pro-studentov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "narodnie\n",
      "pro-studentov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "pro-detey\n",
      "pro-militsiyu\n",
      "pro-vovochku\n",
      "pro-militsiyu\n",
      "pro-militsiyu\n",
      "raznie\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "pro-militsiyu\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-detey\n",
      "poshlie-i-intimnie\n",
      "aforizmi\n",
      "raznie\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "narodnie\n",
      "aforizmi\n",
      "poshlie-i-intimnie\n",
      "pro-militsiyu\n",
      "raznie\n",
      "raznie\n",
      "pro-studentov\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "pro-detey\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-studentov\n",
      "pro-alkogolikov\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-militsiyu\n",
      "pro-studentov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "pro-studentov\n",
      "narodnie\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "raznie\n",
      "aforizmi\n",
      "pro-studentov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "raznie\n",
      "raznie\n",
      "pro-detey\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-armiu\n",
      "pro-militsiyu\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "aforizmi\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "pro-studentov\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-studentov\n",
      "poshlie-i-intimnie\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "raznie\n",
      "tsitati\n",
      "narodnie\n",
      "pro-militsiyu\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "aforizmi\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "narodnie\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-detey\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "meditsinskie\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-militsiyu\n",
      "pro-detey\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "shkolnie-i-pro-shkolu\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "aforizmi\n",
      "narodnie\n",
      "raznie\n",
      "narodnie\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "narodnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "aforizmi\n",
      "raznie\n",
      "narodnie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "narodnie\n",
      "pro-militsiyu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "narodnie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "shkolnie-i-pro-shkolu\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "aforizmi\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "shkolnie-i-pro-shkolu\n",
      "aforizmi\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "shkolnie-i-pro-shkolu\n",
      "narodnie\n",
      "aforizmi\n",
      "raznie\n",
      "pro-militsiyu\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-militsiyu\n",
      "pro-militsiyu\n",
      "pro-studentov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-detey\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "pro-armiu\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "aforizmi\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "narodnie\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "narodnie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-studentov\n",
      "aforizmi\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "pro-studentov\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "pro-militsiyu\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "raznie\n",
      "tsitati\n",
      "narodnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "pro-detey\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "aforizmi\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "aforizmi\n",
      "raznie\n",
      "pro-detey\n",
      "narodnie\n",
      "pro-militsiyu\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "tsitati\n",
      "poshlie-i-intimnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "aforizmi\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "shkolnie-i-pro-shkolu\n",
      "aforizmi\n",
      "pro-studentov\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-studentov\n",
      "pro-studentov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-detey\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-studentov\n",
      "shkolnie-i-pro-shkolu\n",
      "narodnie\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "aforizmi\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "aforizmi\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-studentov\n",
      "pro-militsiyu\n",
      "raznie\n",
      "narodnie\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "shkolnie-i-pro-shkolu\n",
      "poshlie-i-intimnie\n",
      "aforizmi\n",
      "pro-studentov\n",
      "narodnie\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-militsiyu\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "aforizmi\n",
      "shkolnie-i-pro-shkolu\n",
      "shkolnie-i-pro-shkolu\n",
      "narodnie\n",
      "narodnie\n",
      "narodnie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "aforizmi\n",
      "pro-armiu\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "poshlie-i-intimnie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "pro-studentov\n",
      "raznie\n",
      "raznie\n",
      "pro-studentov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-detey\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "poshlie-i-intimnie\n",
      "aforizmi\n",
      "pro-armiu\n",
      "pro-militsiyu\n",
      "pro-militsiyu\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "pro-militsiyu\n",
      "narodnie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "poshlie-i-intimnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-militsiyu\n",
      "poshlie-i-intimnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "aforizmi\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-armiu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-detey\n",
      "narodnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-studentov\n",
      "raznie\n",
      "narodnie\n",
      "aforizmi\n",
      "pro-studentov\n",
      "pro-detey\n",
      "pro-militsiyu\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-armiu\n",
      "pro-studentov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "pro-militsiyu\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "pro-studentov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "raznie\n",
      "pro-militsiyu\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "pro-militsiyu\n",
      "pro-studentov\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "narodnie\n",
      "narodnie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "shkolnie-i-pro-shkolu\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-studentov\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "aforizmi\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "narodnie\n",
      "aforizmi\n",
      "aforizmi\n",
      "narodnie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-militsiyu\n",
      "narodnie\n",
      "shkolnie-i-pro-shkolu\n",
      "narodnie\n",
      "narodnie\n",
      "pro-militsiyu\n",
      "meditsinskie\n",
      "raznie\n",
      "aforizmi\n",
      "shkolnie-i-pro-shkolu\n",
      "aforizmi\n",
      "pro-armiu\n",
      "aforizmi\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-detey\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "raznie\n",
      "aforizmi\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "aforizmi\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "pro-studentov\n",
      "raznie\n",
      "pro-studentov\n",
      "aforizmi\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-detey\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "narodnie\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "pro-studentov\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "narodnie\n",
      "pro-militsiyu\n",
      "pro-vovochku\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-detey\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "aforizmi\n",
      "pro-detey\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-militsiyu\n",
      "pro-studentov\n",
      "pro-militsiyu\n",
      "raznie\n",
      "pro-militsiyu\n",
      "narodnie\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "aforizmi\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "aforizmi\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "aforizmi\n",
      "narodnie\n",
      "narodnie\n",
      "raznie\n",
      "narodnie\n",
      "raznie\n",
      "narodnie\n",
      "meditsinskie\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "narodnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-detey\n",
      "poshlie-i-intimnie\n",
      "pro-militsiyu\n",
      "narodnie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "narodnie\n",
      "narodnie\n",
      "raznie\n",
      "shkolnie-i-pro-shkolu\n",
      "shkolnie-i-pro-shkolu\n",
      "aforizmi\n",
      "aforizmi\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "pro-detey\n",
      "aforizmi\n",
      "raznie\n",
      "raznie\n",
      "narodnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "aforizmi\n",
      "narodnie\n",
      "raznie\n",
      "raznie\n",
      "pro-militsiyu\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "aforizmi\n",
      "shkolnie-i-pro-shkolu\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-vovochku\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-armiu\n",
      "raznie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "pro-militsiyu\n",
      "raznie\n",
      "raznie\n",
      "meditsinskie\n",
      "pro-mugchin\n",
      "shkolnie-i-pro-shkolu\n",
      "raznie\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "aforizmi\n",
      "shkolnie-i-pro-shkolu\n",
      "poshlie-i-intimnie\n",
      "pro-armiu\n",
      "pro-militsiyu\n",
      "poshlie-i-intimnie\n",
      "poshlie-i-intimnie\n",
      "pro-alkogolikov\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "raznie\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "raznie\n",
      "narodnie\n",
      "poshlie-i-intimnie\n",
      "meditsinskie\n",
      "pro-armiu\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "narodnie\n",
      "pro-alkogolikov\n",
      "pro-evreev\n",
      "raznie\n",
      "raznie\n",
      "narodnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "raznie\n",
      "pro-militsiyu\n",
      "poshlie-i-intimnie\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "raznie\n",
      "pro-detey\n",
      "narodnie\n",
      "aforizmi\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "pro-armiu\n",
      "poshlie-i-intimnie\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "aforizmi\n",
      "shkolnie-i-pro-shkolu\n",
      "pro-alkogolikov\n",
      "pro-militsiyu\n",
      "pro-militsiyu\n",
      "pro-armiu\n",
      "pro-militsiyu\n",
      "narodnie\n",
      "aforizmi\n",
      "raznie\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "narodnie\n",
      "narodnie\n",
      "narodnie\n",
      "pro-studentov\n",
      "pro-alkogolikov\n",
      "poshlie-i-intimnie\n",
      "raznie\n"
     ]
    }
   ],
   "source": [
    "model_classification.load_state_dict(torch.load('weights_mlm_classification_20000_1024_no_tsitati.pt'))\n",
    "df = pd.read_csv('m18_jokes_dataset.csv')\n",
    "data = df['text'].sample(1000)\n",
    "preds_mix = [model_classification.predict(text, device=\"cuda\") for text in data]\n",
    "d = dict(zip(data, [classes[pred] for pred in preds_mix]))\n",
    "dd = pd.DataFrame(list(d.items()), columns=[\"текст\", \"тема\"])\n",
    "# склонность присваивать цитаты ушла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abf0ccf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "тема\n",
       "raznie                   259\n",
       "pro-alkogolikov          231\n",
       "narodnie                 135\n",
       "pro-militsiyu             84\n",
       "aforizmi                  83\n",
       "shkolnie-i-pro-shkolu     66\n",
       "poshlie-i-intimnie        53\n",
       "pro-studentov             36\n",
       "pro-detey                 25\n",
       "pro-armiu                 13\n",
       "meditsinskie               5\n",
       "pro-vovochku               4\n",
       "tsitati                    3\n",
       "pro-mugchin                2\n",
       "pro-evreev                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd['тема'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354cbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "narodnie\n",
      "meditsinskie\n",
      "pro-alkogolikov\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 1, 4]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['Черчилль спрашивает Сталина что бы вы с вашими солдатами сделали с Гитлером, будь он у вас в руках? Сталин отвечает раскалил бы кочергу докрасна и засунул бы холодным концом ему в задницу. -А почему холодным, товарищ Сталин? -Чтобы вы, господин Черчилль, не помогли ему ее вытащить',\n",
    "        'Приходит пациент к доктору и говорит: доктор, хуй чешется. Доктор отвечает: мой чаще. -Нет, мой',\n",
    "        \"— Вовочка, ты почему не сделал школьное домашнее задание? — А я в шахматы играл с папой.\"]\n",
    "[model_classification.predict(text, device=\"cuda\") for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca39c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dill\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, vocab_path):\n",
    "        self.vocab = dill.load(open(vocab_path, \"rb\"))\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.sep_token = \"<SEP>\"\n",
    "        self.cls_token = \"<CLS>\"\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "        self.sep_token_id = self.vocab[self.sep_token]\n",
    "        self.cls_token_id = len(self.vocab)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        import nltk\n",
    "        import pymorphy3\n",
    "        from nltk.corpus import stopwords\n",
    "        nltk.download(\"stopwords\")\n",
    "        morph = pymorphy3.MorphAnalyzer()\n",
    "        stop_words = set(stopwords.words(\"russian\")) - {'не', 'ну', 'вот'}\n",
    "        tokens = word_tokenize(text, language=\"russian\")\n",
    "        sep_tokens = {\".\", \"?\", \"!\", \"...\"}\n",
    "        lemmas = []\n",
    "        for token in tokens:\n",
    "            if token in sep_tokens:\n",
    "                lemmas.append(\"<SEP>\")\n",
    "            elif token.isalpha() and token not in stop_words:\n",
    "                lemmas.append(morph.parse(token)[0].normal_form)\n",
    "        return \" \".join(lemmas)\n",
    "\n",
    "    def __call__(self, texts, max_length=None, padding=True, truncation=True, return_tensors=\"pt\"):\n",
    "        if isinstance(texts, pd.DataFrame):\n",
    "            texts = texts.iloc[:, 0].tolist()\n",
    "        elif isinstance(texts, pd.Series):\n",
    "            texts = texts.tolist()\n",
    "        elif not isinstance(texts, list):\n",
    "            raise TypeError(\"На вход должен подаваться list, Series или DataFrame с одним столбцом\")\n",
    "\n",
    "        processed = [self.preprocess(t) for t in texts]\n",
    "        lengths = [len(s.split()) for s in processed]\n",
    "        quantile99 = int(pd.DataFrame(lengths).quantile(0.99))\n",
    "        max_length = max_length or quantile99\n",
    "\n",
    "        token_ids = []\n",
    "        attn_masks = []\n",
    "\n",
    "        for text in processed:\n",
    "            ids = [self.vocab.get(tok, self.unk_token_id) for tok in text.split()]\n",
    "            if truncation and len(ids) > max_length:\n",
    "                ids = ids[:max_length]\n",
    "            if padding:\n",
    "                pad_len = max_length - len(ids)\n",
    "                ids = ids + [self.pad_token_id] * pad_len\n",
    "            mask = [1 if i != self.pad_token_id else 0 for i in ids]\n",
    "            token_ids.append(ids)\n",
    "            attn_masks.append(mask)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(token_ids),\n",
    "            \"attention_mask\": torch.tensor(attn_masks)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "184f8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dill\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class CustomTokenizer_MLM:\n",
    "    def __init__(self, vocab_path):\n",
    "        self.vocab = dill.load(open(vocab_path, \"rb\"))\n",
    "        self.vocab['<MASK>'] = len(self.vocab)\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.mask_token = \"<MASK>\"\n",
    "        self.sep_token = \"<SEP>\"\n",
    "        self.cls_token = \"<CLS>\"\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "        self.mask_token_id = self.vocab[self.mask_token]\n",
    "        self.sep_token_id = self.vocab[self.sep_token]\n",
    "        self.cls_token_id = len(self.vocab)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        import nltk\n",
    "        import pymorphy3\n",
    "        from nltk.corpus import stopwords\n",
    "        nltk.download(\"stopwords\")\n",
    "        morph = pymorphy3.MorphAnalyzer()\n",
    "        stop_words = set(stopwords.words(\"russian\")) - {'не', 'ну', 'вот'}\n",
    "        text = re.sub(r\"<MASK>\", \" MASKTOKEN \", text)\n",
    "        text = re.sub(r\"<SEP>\", \" SEPTOKEN \", text)\n",
    "        tokens = word_tokenize(text, language=\"russian\")\n",
    "        sep_tokens = {\".\", \"?\", \"!\", \"...\"}\n",
    "        lemmas = []\n",
    "        for token in tokens:\n",
    "            if token == \"MASKTOKEN\":\n",
    "                lemmas.append(\"<MASK>\")\n",
    "            elif token in sep_tokens or token == \"SEPTOKEN\":\n",
    "                lemmas.append(\"<SEP>\")\n",
    "            elif token.isalpha() and token not in stop_words:\n",
    "                lemmas.append(morph.parse(token)[0].normal_form)\n",
    "        return \" \".join(lemmas)\n",
    "\n",
    "    def __call__(self, texts, max_length=None, padding=True, truncation=True, return_tensors=\"pt\"):\n",
    "        if isinstance(texts, pd.DataFrame):\n",
    "            texts = texts.iloc[:, 0].tolist()\n",
    "        elif isinstance(texts, pd.Series):\n",
    "            texts = texts.tolist()\n",
    "        elif not isinstance(texts, list):\n",
    "            raise TypeError(\"На вход должен подаваться list, Series или DataFrame с одним столбцом\")\n",
    "\n",
    "        processed = [self.preprocess(t) for t in texts]\n",
    "        lengths = [len(s.split()) for s in processed]\n",
    "        quantile99 = int(pd.DataFrame(lengths).quantile(0.99))\n",
    "        max_length = max_length or quantile99\n",
    "\n",
    "        token_ids = []\n",
    "        attn_masks = []\n",
    "\n",
    "        for text in processed:\n",
    "            ids = [self.vocab.get(tok, self.unk_token_id) for tok in text.split()]\n",
    "            if truncation and len(ids) > max_length:\n",
    "                ids = ids[:max_length]\n",
    "            if padding:\n",
    "                pad_len = max_length - len(ids)\n",
    "                ids = ids + [self.pad_token_id] * pad_len\n",
    "            mask = [1 if i != self.pad_token_id else 0 for i in ids]\n",
    "            token_ids.append(ids)\n",
    "            attn_masks.append(mask)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(token_ids),\n",
    "            \"attention_mask\": torch.tensor(attn_masks)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0243a046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  18,  318, 3340,   26,   62],\n",
      "        [  47,   62,    2,    0,    0]]) tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\wa-\n",
      "[nltk_data]     Arakelyan_P\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\wa-\n",
      "[nltk_data]     Arakelyan_P\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\wa-Arakelyan_P\\AppData\\Local\\Temp\\ipykernel_9932\\277918693.py:46: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  quantile99 = int(pd.DataFrame(lengths).quantile(0.99))\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer(\"anek_2ch_vocab_20000.pkl\")\n",
    "batch = tokenizer([\"Я сегодня спал весь день.\", \"Хороший день!\"])\n",
    "print(batch[\"input_ids\"], batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148e401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
